# Arabic Sentiment Analysis using Transformers

A from-scratch implementation of a Transformer model in PyTorch to perform sentiment analysis on Arabic customer reviews. This repo guides you through data scraping, preprocessing, translation, model building, training, and evaluation.

## Content
Features
Repository Structure
Usage
Results
Future Work
Authors
License

## Features

- End-to-end pipeline from web scraping to model evaluation
- 
- Custom Transformer built from scratch (positional encodings, multi-head attention, encoder layers)
- 
- BPE tokenizer trained on your data for robust subword representations
- 
- English→Arabic translation using Maarefa-NLP before Arabic preprocessing
- 
- Comprehensive EDA on both English and Arabic datasets
  
## Repositry Structure

```
.
├── notebooks/
│   ├── 01_Scraping.ipynb             # Scrape English reviews from Trustpilot
│   ├── 02_PreProcessingEnglish.ipynb # Clean & normalize English text
│   ├── 03_Translation.ipynb          # Translate to Arabic with Maarefa-NLP
│   ├── 04_PreProcessingArabic.ipynb  # Arabic text cleaning & tokenization
│   └── 05_Transformer_Model.ipynb    # Build, train & evaluate transformer
├── data/                             # CSVs generated by notebooks
├── src/
│   ├── tokenizer.py                  # BPE tokenizer training & usage
│   └── transformer.py                # Custom Transformer implementation
├── models/                           # Saved model checkpoints
├── requirements.txt
└── README.md

```

## Usage 
### Scraping
Run 01_Scraping.ipynb to collect English reviews from Trustpilot and save to data/english_reviews.csv.

### English Preprocessing
Run 02_PreProcessingEnglish.ipynb to clean and normalize the raw English data.

###Translation
Run 03_Translation.ipynb to translate the cleaned English reviews into Arabic using Maarefa-NLP.

###Arabic Preprocessing
Run 04_PreProcessingArabic.ipynb to clean, remove stopwords, stem, and prepare Arabic text for tokenization.

###Model Training
Run 05_Transformer_Model.ipynb to train your custom Transformer on the preprocessed Arabic dataset, then evaluate.

## Results 
Accuracy: 84.87%

Precision: 84.97%

Recall: 84.87%

F1-Score: 84.91%

These metrics demonstrate that a custom-built Transformer can rival standard architectures on Arabic sentiment tasks.

## Future Work
Hyperparameter tuning (learning rates, number of heads, layers)

Expand dataset with additional Arabic reviews for dialectal robustness

Deploy as a real-time API pipeline for live customer feedback

Extend to other languages for multilingual sentiment analysis
